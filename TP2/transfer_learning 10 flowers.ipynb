{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer_learning 10 flowers.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1dm6Uchlbqrf2FdzhP5S-0CvpEuQAzUwW","authorship_tag":"ABX9TyMUu8SYXgfXhudLxpCRrSOx"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"njEh8Cemo3zb","executionInfo":{"status":"ok","timestamp":1632436877147,"user_tz":180,"elapsed":7956,"user":{"displayName":"Fernando Das Neves","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06324247755041763216"}},"outputId":"be0c2604-dbac-45ce-9727-223443d42064"},"source":[" # Antes de ejecutar: Activar GPUs como sigue:\n"," # menu \"Entorno de Ejecucion\" -> \"Cambiar tipo de entorno de ejecucion\" -> \"Acelerador de Hardware\" = \"GPU\"\n"," # How to save tensorflow model to google drive: https://stackoverflow.com/questions/67305778/how-to-save-tensorflow-model-to-google-drive\n","\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","print(\"Usandor Tensorflow version \" + tf.__version__)\n","\n","\n","if tf.test.gpu_device_name():\n","  print('Usando GPU: {}'.format(tf.test.gpu_device_name()))\n","else:\n","  print(\"Usando CPU.\")"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Usandor Tensorflow version 2.6.0\n","Usando GPU: /device:GPU:0\n"]}]},{"cell_type":"code","metadata":{"id":"IFNd6dlwv7T1","executionInfo":{"status":"ok","timestamp":1632436878540,"user_tz":180,"elapsed":1406,"user":{"displayName":"Fernando Das Neves","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06324247755041763216"}}},"source":["##################################################################\n","# Este script carga VGG16, reemplaza la ultima capa de prediccion,\n","# y reentrena para clasificar imagenes de 10 categorias de flores.\n","##################################################################\n","\n","import h5py\n","from skimage.transform import resize\n","import numpy as np\n","\n","from keras.layers import Flatten, Dense, Dropout, Input, Conv2D, MaxPooling2D\n","from keras.models import Model, Sequential\n","from tensorflow.keras.optimizers import SGD\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.utils.vis_utils import plot_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n","import sklearn.preprocessing\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"fm-hHB0NyDux","executionInfo":{"status":"ok","timestamp":1632436878545,"user_tz":180,"elapsed":38,"user":{"displayName":"Fernando Das Neves","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06324247755041763216"}}},"source":["# algunos parametros del entrenamiento\n","batch_size = 32 # cada batch son 32 imagenes\n","epochs = 13 # entrenamos hasta 13 epochs (pasadas sobre el dataset de entrenamiento), a menos que paremos antes por early stopping\n","epochs_to_stop_after_no_improvement = 3 # cuantas epochs consecutivas no tienen que tener mejora para aplicar early stopping\n","num_cores = 4 # cambiar este numero a la cantidad de cores de su cpu\n","\n","# parametros del descenso de gradiente\n","learning_rate=0.001\n","learning_rate_decay=1e-6\n","learning_rate_momentum=0.7"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"iI_H5qI_yF9w","executionInfo":{"status":"ok","timestamp":1632436878547,"user_tz":180,"elapsed":36,"user":{"displayName":"Fernando Das Neves","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06324247755041763216"}}},"source":["def preprocesar_imagen_como_caffe(image:np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Transforma las imagenes al formato con el que fue entrenado el modelo de VGG16 que estamos usando.\n","    :param image: Una imagen representada como una matriz de (largo en pixels, alto en pixels, 3 canales)\n","    :return La imagen transformada.\n","    \"\"\"\n","    # pasar imagen de  'RGB'->'BGR', porque el modelo ya entrenado de VGG16 que estamos usando proviene de Caffe, y fue entrenado en ese orden de channels\n","    image = image[:, :, ::-1]\n","    # central valor de los pixels alrededor del valor medio de cada canal en el conj. de entrenamiento,\n","    # esto se calcula simplemente promediando todos los valores de cada canal en todas las imagenes de entrenamiento en imagenet.\n","    image[:, :, 0] -= 103.939\n","    image[:, :, 1] -= 116.779\n","    image[:, :, 2] -= 123.68\n","    return image\n","\n","\n","def rescalar_imagenes(flower_images:np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Cambia el tamaño de las imagenes de flores al tamaño con el que esta entrenada VGG16: 224x224 pixels\n","    :param flower_images: Una matriz de (nro_imagenes, ancho en pixels, alto en pixels, 3 canales); cada valor de la matriz esta entre 0 y 255.\n","    :return: Otra matriz de las mismas dimensiones de 'flowers' pero con todos los valores entre 0 y 1.\n","    \"\"\"\n","    rescaled_images = np.empty((flower_images.shape[0], 224, 224, flower_images.shape[3]), dtype=flower_images.dtype)\n","    for i in range(0, flower_images.shape[0]):\n","        rescaled_images[i, 0:224, 0:224, ] = resize(flower_images[i] / 255.0, (224, 224), anti_aliasing=True) * 255.0\n","    return rescaled_images\n","\n","\n","def encode_onehot_labels(labels:np.ndarray) ->np.ndarray:\n","    \"\"\"\n","     Cambia la codificacion de las categorias de flores a one-hot encoding, que es lo que necesitamos para entrenar la NN.\n","     :param labels: Una lista o arreglo de strings, el i-avo string es la categoria de la i-ava imagen de entrenamiento.\n","     :return una matriz de tamaño (cantidad de ejemplos en labels, cantidad de categorias en 'labels') donde cada celda es 1 o 0. Hay 1 solo 1 por fila.\n","    \"\"\"\n","    label_binarizer = sklearn.preprocessing.LabelBinarizer()\n","    label_binarizer.fit(range(max(labels) + 1))\n","    return label_binarizer.transform(labels)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Emb12Xn9yLhM","executionInfo":{"status":"ok","timestamp":1632436878549,"user_tz":180,"elapsed":34,"user":{"displayName":"Fernando Das Neves","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06324247755041763216"}}},"source":["def VGG_16():\n","    \"\"\"\n","    Crea una red para clasificar imagenes con la arquitectura VGG16, para poder reusar los pesos de VGG16 entrenado con imagenet.\n","    \"\"\"\n","    img_input = Input(shape=(224,224,3)) # tamaño de imagenes es 224x224 y 3 canales de colores\n","\n","    # Block 1\n","    output = Conv2D(64, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block1_conv1')(img_input)\n","    output = Conv2D(64, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block1_conv2')(output)\n","    output = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(output)\n","\n","    # Block 2\n","    output = Conv2D(128, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block2_conv1')(output)\n","    output = Conv2D(128, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block2_conv2')(output)\n","    output = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(output)\n","\n","    # Block 3\n","    output = Conv2D(256, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block3_conv1')(output)\n","    output = Conv2D(256, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block3_conv2')(output)\n","    output = Conv2D(256, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block3_conv3')(output)\n","    output = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(output)\n","\n","    # Block 4\n","    output = Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block4_conv1')(output)\n","    output = Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block4_conv2')(output)\n","    output = Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block4_conv3')(output)\n","    output = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(output)\n","\n","    # Block 5\n","    output = Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block5_conv1')(output)\n","    output = Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block5_conv2')(output)\n","    output = Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block5_conv3')(output)\n","    output = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(output)\n","\n","    # capa de clasificacion\n","    output = Flatten(name='flatten')(output)\n","    output = Dense(4096, activation='relu', name='fc1')(output)\n","    output = Dense(4096, activation='relu', name='fc2')(output)\n","    output = Dense(1000, activation='softmax', name='predictions')(output) # la salida son las 1000 categorias de imagenet\n","\n","    return Model(inputs=img_input, outputs=output, name='vgg16')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tv7VlazpyORO","executionInfo":{"status":"ok","timestamp":1632436884228,"user_tz":180,"elapsed":3441,"user":{"displayName":"Fernando Das Neves","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06324247755041763216"}}},"source":["# leer el dataset de 210 imagenes de flores junto con sus etiquetas\n","dataset = h5py.File(\"/content/drive/MyDrive/collab/transfer_learning/imagenet_dataset/10FlowerColorImages.h5\",'r')\n","# El [()] indica que lea el dataset completo a memoria, en vez de leerlo bajo demanda\n","images = dataset['images'][()]\n","image_labels = dataset['labels'][()]\n","images = rescalar_imagenes(images)\n","onehot_labels = encode_onehot_labels(image_labels)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXfOOjk9y557","executionInfo":{"status":"ok","timestamp":1632436884804,"user_tz":180,"elapsed":582,"user":{"displayName":"Fernando Das Neves","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06324247755041763216"}}},"source":["# armar la arquitectura de la red neuronal\n","pretrained_vgg16 = VGG_16()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJdWYYMhy8NE","executionInfo":{"status":"ok","timestamp":1632436895105,"user_tz":180,"elapsed":9211,"user":{"displayName":"Fernando Das Neves","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06324247755041763216"}}},"source":["# Leer los pesos de VGG16 ya entrenada con imagenet.\n","# El modelo fue entrenado con los canales de cada imagen en el orden 'BGR' que utiliza la biblitoeca \"Caffe\",\n","#  y c/pixel centrado sobre la media del dataset imagenet = [103.939, 116.779, 123.68]\n","# Las imagenes nuevas tienen que tener exactamente esta transformacion.\n","pretrained_vgg16.load_weights('/content/drive/MyDrive/collab/transfer_learning/imagenet_dataset/vgg16_weights.h5')"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"3096j1L7yVfx","executionInfo":{"status":"ok","timestamp":1632436895105,"user_tz":180,"elapsed":33,"user":{"displayName":"Fernando Das Neves","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06324247755041763216"}}},"source":["# setear a todas las capas, excepto la ultima de clasificacion \n","# como \"no entrenable\" (los pesos no se actualizaran)\n","for layer in pretrained_vgg16.layers[:-1]:\n","    layer.trainable = False\n","\n","# Descartamos la ultima capa de vgg16 y creamos una capa nueva, cuya entrada es la anteultima capa (\"fc2\") de vgg16.\n","# Necesitamos hacer esto porque el numero de categorias en el nuevo dataset es diferente al numero de categorias en imagenet.\n","new_layer = Dense(10, activation=\"softmax\", name=\"predict_10flowers\")(pretrained_vgg16.get_layer('fc2').output)\n","\n","# crear un nuevo modelo cuya salida es la nueva capa\n","new_model = Model(inputs=pretrained_vgg16.input, outputs=new_layer)\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"5q_Jonghyesi","executionInfo":{"status":"ok","timestamp":1632436895106,"user_tz":180,"elapsed":31,"user":{"displayName":"Fernando Das Neves","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06324247755041763216"}}},"source":["# compilar el modelo con SGD/momentum optimizer\n","# y un learning rate muuuuy lento\n","sgd = SGD(learning_rate=learning_rate, decay=learning_rate_decay, momentum=learning_rate_momentum, nesterov=True)\n","# regla en Keras:\n","# si loss=categorical_crossentropy => metrics=categorical_accuracy\n","# si loss=binary_crossentropy y mas de 2 clases => metrics=categorical_accuracy, sino metrics=binary_accuracy\n","new_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"2iSzocdpyjei","executionInfo":{"status":"ok","timestamp":1632436895106,"user_tz":180,"elapsed":30,"user":{"displayName":"Fernando Das Neves","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06324247755041763216"}}},"source":["X_train, X_test, y_train, y_test = train_test_split(images, onehot_labels, train_size=0.7, test_size=0.3, shuffle=True, stratify=image_labels)\n","\n","# aumentar la cantidad de ejemplos de entrenamiento, inventando variaciones de las imagenes\n","train_datagen = ImageDataGenerator(\n","    width_shift_range = 0.1,\n","    height_shift_range=0.1,\n","    shear_range=0.2,\n","    zoom_range=[0.8, 1.4],\n","    horizontal_flip=True,\n","    rotation_range=10,\n","    preprocessing_function=preprocesar_imagen_como_caffe\n",")\n","\n","# testear con los casos de prueba sin rotar, cambiar tamaño ni nada, solo acomodados a la manera en que fue entrenada la NN\n","test_datagen = ImageDataGenerator(preprocessing_function=preprocesar_imagen_como_caffe)\n","\n","train_generator = train_datagen.flow(\n","    X_train,\n","    y_train,\n","    batch_size=batch_size,\n",")\n","\n","validation_generator = test_datagen.flow(\n","    X_test,\n","    y_test\n",")"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFgMzQNoymQm","executionInfo":{"status":"ok","timestamp":1632437046203,"user_tz":180,"elapsed":151126,"user":{"displayName":"Fernando Das Neves","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06324247755041763216"}},"outputId":"7ae1cb08-ff3a-458e-d4cc-1fc9612b91b2"},"source":["# entrenar y guarda el mejor resultado\n","new_model.fit(\n","    train_generator,\n","    steps_per_epoch=len(y_train) // batch_size,\n","    epochs=epochs,\n","    validation_data=validation_generator,\n","    validation_steps=len(y_test) // batch_size,\n","    use_multiprocessing=True,\n","    workers=num_cores,\n","    # parar el entrenamiento si no mejora en 3 epochs consecutivas; guardar el mejor modelo hasta ese momento al final de cada epoch.\n","    # Aqui, la medida de \"mejor modelo\" es val_categorical_accuracy = accuracy en el dataset de test\n","    callbacks=[EarlyStopping(monitor='val_categorical_accuracy', patience=epochs_to_stop_after_no_improvement, verbose=1),\n","               ModelCheckpoint('/content/drive/MyDrive/collab/transfer_learning/vgg16_retrained_10flowers.h5', verbose=1, monitor='val_categorical_accuracy', save_best_only=True, mode='auto')]\n",")\n","\n","print(\"Entrenamiento finalizado.\")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/26\n","4/4 [==============================] - 50s 4s/step - loss: 2.8990 - categorical_accuracy: 0.1652 - val_loss: 1.4351 - val_categorical_accuracy: 0.4688\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.46875, saving model to /content/drive/MyDrive/collab/transfer_learning/vgg16_retrained_10flowers.h5\n","Epoch 2/26\n","4/4 [==============================] - 3s 482ms/step - loss: 1.3616 - categorical_accuracy: 0.5217 - val_loss: 1.0592 - val_categorical_accuracy: 0.6250\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.46875 to 0.62500, saving model to /content/drive/MyDrive/collab/transfer_learning/vgg16_retrained_10flowers.h5\n","Epoch 3/26\n","4/4 [==============================] - 3s 551ms/step - loss: 0.5359 - categorical_accuracy: 0.8609 - val_loss: 0.9153 - val_categorical_accuracy: 0.6250\n","\n","Epoch 00003: val_categorical_accuracy did not improve from 0.62500\n","Epoch 4/26\n","4/4 [==============================] - 3s 471ms/step - loss: 0.4087 - categorical_accuracy: 0.8522 - val_loss: 0.7762 - val_categorical_accuracy: 0.7812\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.62500 to 0.78125, saving model to /content/drive/MyDrive/collab/transfer_learning/vgg16_retrained_10flowers.h5\n","Epoch 5/26\n","4/4 [==============================] - 3s 475ms/step - loss: 0.3262 - categorical_accuracy: 0.9391 - val_loss: 0.6524 - val_categorical_accuracy: 0.8125\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.78125 to 0.81250, saving model to /content/drive/MyDrive/collab/transfer_learning/vgg16_retrained_10flowers.h5\n","Epoch 6/26\n","4/4 [==============================] - 3s 489ms/step - loss: 0.2589 - categorical_accuracy: 0.9565 - val_loss: 0.4014 - val_categorical_accuracy: 0.8438\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.81250 to 0.84375, saving model to /content/drive/MyDrive/collab/transfer_learning/vgg16_retrained_10flowers.h5\n","Epoch 7/26\n","4/4 [==============================] - 4s 724ms/step - loss: 0.2001 - categorical_accuracy: 0.9609 - val_loss: 0.5217 - val_categorical_accuracy: 0.8750\n","\n","Epoch 00007: val_categorical_accuracy improved from 0.84375 to 0.87500, saving model to /content/drive/MyDrive/collab/transfer_learning/vgg16_retrained_10flowers.h5\n","Epoch 8/26\n","4/4 [==============================] - 3s 472ms/step - loss: 0.1886 - categorical_accuracy: 0.9652 - val_loss: 0.6914 - val_categorical_accuracy: 0.8438\n","\n","Epoch 00008: val_categorical_accuracy did not improve from 0.87500\n","Epoch 9/26\n","4/4 [==============================] - 3s 484ms/step - loss: 0.1203 - categorical_accuracy: 0.9922 - val_loss: 0.6288 - val_categorical_accuracy: 0.8125\n","\n","Epoch 00009: val_categorical_accuracy did not improve from 0.87500\n","Epoch 10/26\n","4/4 [==============================] - 4s 497ms/step - loss: 0.1325 - categorical_accuracy: 0.9913 - val_loss: 0.3875 - val_categorical_accuracy: 0.9062\n","\n","Epoch 00010: val_categorical_accuracy improved from 0.87500 to 0.90625, saving model to /content/drive/MyDrive/collab/transfer_learning/vgg16_retrained_10flowers.h5\n","Epoch 11/26\n","4/4 [==============================] - 3s 478ms/step - loss: 0.0871 - categorical_accuracy: 1.0000 - val_loss: 0.5798 - val_categorical_accuracy: 0.8438\n","\n","Epoch 00011: val_categorical_accuracy did not improve from 0.90625\n","Epoch 12/26\n","4/4 [==============================] - 3s 493ms/step - loss: 0.1060 - categorical_accuracy: 0.9913 - val_loss: 0.4269 - val_categorical_accuracy: 0.8750\n","\n","Epoch 00012: val_categorical_accuracy did not improve from 0.90625\n","Epoch 13/26\n","4/4 [==============================] - 4s 506ms/step - loss: 0.0809 - categorical_accuracy: 1.0000 - val_loss: 0.4069 - val_categorical_accuracy: 0.9062\n","\n","Epoch 00013: val_categorical_accuracy did not improve from 0.90625\n","Epoch 00013: early stopping\n","Entrenamiento finalizado.\n"]}]}]}